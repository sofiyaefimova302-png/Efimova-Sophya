{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sofiyaefimova302-png/Efimova-Sophya/blob/main/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22hw_tokenization__ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXdJjBa9sCq"
      },
      "source": [
        "# Домашнее задание: Токенизация текста"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Дан список текстов, которые нужно токенизировать разными способами"
      ],
      "metadata": {
        "id": "1xVbvaj_phyN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\n",
        "\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\",\n",
        "\"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\",\n",
        "\"I can't believe she's going! Let's meet at Jane's house. They'll love it.\",\n",
        "\"What's the ETA for the package? Please e-mail support@example.com ASAP!\"\n",
        "]"
      ],
      "metadata": {
        "id": "uj-xaNnwpiPo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " Используйте способы токенизации, которые описаны в практикуме. Каждый способ нужно обернуть в функцию, например:\n",
        "\n",
        " ```python\n",
        " def simple_tokenization(string):\n",
        "   return string.split()\n",
        "   ```"
      ],
      "metadata": {
        "id": "ix1Im4Kcqb3_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Напишите функцию для токенизации по пробелам и знакам препинания (используйте оператор `def`)"
      ],
      "metadata": {
        "id": "Ih0BBOGBpv6Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#импортирую модуль для работы с регулярными выражениями\n",
        "import re\n",
        "#определяю переменную text\n",
        "text = [\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\", \"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\", \"I can't believe she's going! Let's meet at Jane's house. They'll love it.\", \"What's the ETA for the package? Please e-mail support@example.com ASAP!\"]\n",
        "#определяю функции токенизации текста\n",
        "def function_1(text):\n",
        "   #создаю пустой список для хранения результата токенизации\n",
        "  tokenized_text = []\n",
        "  #перебираю каждый элемент (строку текста) в списке\n",
        "  for expression in text:\n",
        "    token = re.findall(r'\\w+|[^\\w\\s]', expression) #[^\\w\\s] - находит отдельные символы (запятые, восклицательные знаки и т.д.)\n",
        "    tokenized_text.extend(token) #extend() - добавляет элементы списка\n",
        "  return tokenized_text\n",
        "print(function_1(text))"
      ],
      "metadata": {
        "id": "W1QCaw6cqDnn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbdbbd06-4dc2-43e6-99e5-1e43fad285e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'\", 's', 'a', 'beautiful', 'day', '!', 'Dr', '.', 'Smith', 'arrived', 'at', '5', ':', '30', 'p', '.', 'm', '.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1', ',', '000', '.', '50', '.', 'I', 'can', \"'\", 't', 'believe', 'she', \"'\", 's', 'going', '!', 'Let', \"'\", 's', 'meet', 'at', 'Jane', \"'\", 's', 'house', '.', 'They', \"'\", 'll', 'love', 'it', '.', 'What', \"'\", 's', 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support', '@', 'example', '.', 'com', 'ASAP', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Напишите функцию для токенизации текста с помощью NLTK"
      ],
      "metadata": {
        "id": "GThvPcovqgO5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#импортирую библиотеку NLTK для токенизации\n",
        "import nltk\n",
        "nltk.download('punkt', quiet=\"True\") #quiet=\"True\" использую для того, чтобы в вывод не попали строки с загрузкой\n",
        "nltk.download('punkt_tab', quiet=\"True\")\n",
        "#импортирую функции word_tokenize из модуля nltk.tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "#определяю переменную text\n",
        "text = [\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\", \"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\", \"I can't believe she's going! Let's meet at Jane's house. They'll love it.\", \"What's the ETA for the package? Please e-mail support@example.com ASAP!\"]\n",
        "def function_2(text):\n",
        "  tokenized_text = []\n",
        "  for expression in text:\n",
        "     #использую встроенный токенизатор NLTK для разбиения строки на токены\n",
        "    token = word_tokenize(expression) # word_tokenize - токенизирует слова, знаки препинания и специальные символы\n",
        "    tokenized_text.extend(token)\n",
        "  return tokenized_text\n",
        "print(function_2(text))\n"
      ],
      "metadata": {
        "id": "14BIv33iqrkL",
        "outputId": "0fa41593-5d4d-4d81-b299-55b0fefdf08c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!', 'Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.', 'I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.', 'What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e-mail', 'support', '@', 'example.com', 'ASAP', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Напишите функцию для токенизации текста с помощью Spacy"
      ],
      "metadata": {
        "id": "GxW7ZP6iqwpt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#импортирую библиотеку Spacy для токенизации\n",
        "import spacy\n",
        "#загружаю модель en_core_web_sm\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "#определяю переменную text\n",
        "text = [\"The quick brown fox jumps over the lazy dog. It's a beatiful day!\", \"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\", \"I can't believe she's going! Let's meet at Jane's house. They'll love it.\", \"What's the ETA for the package? Please e-mail support@example.com ASAP!\"]\n",
        "def function_3(text):\n",
        "  tokenized_text = []\n",
        "  for expression in text:\n",
        "#адаптирую код для выбранной библиотеки\n",
        "    doc = nlp(expression)\n",
        "    tokens = [token.text for token in doc]\n",
        "    tokenized_text.extend(tokens)\n",
        "  return tokenized_text\n",
        "print(function_3(text))"
      ],
      "metadata": {
        "id": "B0NQg-VfuFW_",
        "outputId": "a6284f8c-76fa-4ac4-c4ee-11e1e5d60118",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beatiful', 'day', '!', 'Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.', 'I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.', 'What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support@example.com', 'ASAP', '!']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. С помощью цикла `for` примените каждую из написанных функций к каждому тексту из списка `texts`"
      ],
      "metadata": {
        "id": "WmyJfB9wuKkm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lvUmk94MhrL8",
        "outputId": "d46588f0-35d4-47b8-9668-8b9db154b897",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "function 1:['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'\", 's', 'a', 'beautiful', 'day', '!', 'Dr', '.', 'Smith', 'arrived', 'at', '5', ':', '30', 'p', '.', 'm', '.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1', ',', '000', '.', '50', '.', 'I', 'can', \"'\", 't', 'believe', 'she', \"'\", 's', 'going', '!', 'Let', \"'\", 's', 'meet', 'at', 'Jane', \"'\", 's', 'house', '.', 'They', \"'\", 'll', 'love', 'it', '.', 'What', \"'\", 's', 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support', '@', 'example', '.', 'com', 'ASAP', '!']\n",
            "function 2:['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!', 'Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.', 'I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.', 'What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e-mail', 'support', '@', 'example.com', 'ASAP', '!']\n",
            "function 3:['The', 'quick', 'brown', 'fox', 'jumps', 'over', 'the', 'lazy', 'dog', '.', 'It', \"'s\", 'a', 'beautiful', 'day', '!', 'Dr.', 'Smith', 'arrived', 'at', '5:30', 'p.m.', 'from', 'New', 'York', '.', 'The', 'meeting', 'cost', '$', '1,000.50', '.', 'I', 'ca', \"n't\", 'believe', 'she', \"'s\", 'going', '!', 'Let', \"'s\", 'meet', 'at', 'Jane', \"'s\", 'house', '.', 'They', \"'ll\", 'love', 'it', '.', 'What', \"'s\", 'the', 'ETA', 'for', 'the', 'package', '?', 'Please', 'e', '-', 'mail', 'support@example.com', 'ASAP', '!']\n"
          ]
        }
      ],
      "source": [
        "#создаю список функций\n",
        "functions=[function_1,function_2,function_3]\n",
        "#создаю пустой список для токенизированных списков\n",
        "token_list = []\n",
        "#определяю переменную text\n",
        "text = [\"The quick brown fox jumps over the lazy dog. It's a beautiful day!\", \"Dr. Smith arrived at 5:30 p.m. from New York. The meeting cost $1,000.50.\", \"I can't believe she's going! Let's meet at Jane's house. They'll love it.\", \"What's the ETA for the package? Please e-mail support@example.com ASAP!\"]\n",
        "#перебираю функции\n",
        "for function in functions:\n",
        "    token_list.append(function(text))\n",
        "\n",
        "#делаю вывод \"отформатированным\"\n",
        "for j, i in enumerate(token_list):\n",
        "    print(f\"function {j+1}:{i}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iqAgf6sGhrL8"
      },
      "source": [
        "##### Критерии оценки (макс. балл == 5):\n",
        "\n",
        "- Функциональность (до 4 баллов)): Все методы работают корректно (запускаем код, и он работает)\n",
        "- Качество кода (до 1 балла): Чистый, документированный код с обработкой ошибок (кратко описать, что вы дополнили самостоятельно, например, \"добавлена токенизация `spacy`\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Теоретические вопросы (макс. балл == 5; в ведомость выставляется сумма за практику и теорию)\n",
        "\n",
        "Необходимо дать краткие ответы на вопросы по теме \"токенизация\". В сумме длина ответов на вопрос не должна превышать размер вордовской страницы 14 шрифтом."
      ],
      "metadata": {
        "id": "Mwe1Co6MvibX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Почему простое разделение текста по пробелам и знакам препинания часто является недостаточным для современных NLP-задач? Приведите 2-3 конкретных примера, когда деление текста по символам или словам не работает. (2 балла за полный и подробный ответ на вопрос)\n",
        "\n",
        "Потому что простое разделение текста по пробелам и знакам препинания не учитывает лингвистические особенности и контекст. Например, многокомпонентные термины: \"Нью-Йорк\" ≠ \"Нью\" + \"Йорк\", \"machine learning\" ≠ \"machine\" + \"learning\". Также апострофы: \"don't\" ≠ \"do\" + \"n't\", и специфичные домены: химические формулы \"C6H12O6\", хештеги \"#COVID19\".\n",
        "\n",
        "Т.е.токенизация текста по пробелам и знакам препинания часто приводит к тому, что то, что в обычном (естественном) языке считается 1-м токеном, делится на 2 (а иногда и больше) части. Более того, иногда простое разделение по пробелам и знакам препинания может разделить или удалить важные точки, следующие за аббревиатурами: mr., mrs., dr.и т.д., и этого недостаточно для современных NLP-задач.\n",
        "\n",
        "2. Сколько токенов во фразе \"You shall know a word by the company it keeps\" в модели GPT-5? Как вы получили это значение? (1 балл за правильный ответ и ссылку на ресурс, с помощью которого вы узнали эту информацию)\n",
        "\n",
        "В этой фразе 10 токенов. Я получила это значение, обратившись к демо-версии gpt-токенайзера, ссылка на который указана ниже, выбрав модель gpt-5.\n",
        "https://gpt-tokenizer.dev/\n",
        "\n",
        "3. Опишите своими словами работу алгоритма BPE (можно форматировать ответ с использованием списков, 2 балла за корректное описание и ясное изложение ответа)\n",
        "\n",
        "Алгоритм BPE (Byte Pair Encoding) — это метод субсловной токенизации, который автоматически создает словарь токенов, начиная с базовых символов и повторно объединяя наиболее частые пары символов или токенов.\n",
        "1. Начальное разбиение:исходный текст разбивается на отдельные символы. К каждому слову часто добавляется специальный символ (например, </w>) для обозначения конца слова.\n",
        "2. Подсчёт частот пар: алгоритм анализирует корпус текста и находит пары символов (или уже объединённых токенов), которые встречаются вместе чаще всего.\n",
        "3. Объединение пар: самая частая пара объединяется в один новый токен, который добавляется в словарь.\n",
        "4. Повторение: шаги 2–3 повторяются заданное количество раз или до достижения нужного размера словаря.\n",
        "\n",
        "Если рассматривать на примере \"slow slowest\", то это выглядит следующим образом:\n",
        "После начального разбиения (с символом конца слова </w>):\n",
        "s l o w </w> s l o w e s t </w>\n",
        "\n",
        "1 итерация:\n",
        "Частая пара: s l → объединяем в sl\n",
        "Результат: sl o w </w> sl o w e s t </w>\n",
        "\n",
        "2 итерация:\n",
        "Частая пара: sl o → объединяем в slo\n",
        "Результат: slo w </w> slo w e s t </w>\n",
        "\n",
        "3 итерация:\n",
        "Частая пара: slo w → объединяем в slow\n",
        "Результат: slow </w> slow est </w>\n",
        "\n",
        "4 итерация:\n",
        "Частая пара: e s → объединяем в es\n",
        "Результат: slow </w> slow es t </w>\n",
        "\n",
        "5 итерация:\n",
        "Частая пара: es t  объединяем в est\n",
        "Результат: slow </w> slow est </w>"
      ],
      "metadata": {
        "id": "O-rjkncwdEQj"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}